# -*- coding: utf-8 -*-
"""UAS NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ilym4JA5xvwQV-Ea_v30WW6LBAYrATY-

# UAS NLP

## Import Dataset
"""

import pandas as pd
df=pd.read_csv('blogtext.csv')

"""## EDA"""

df.info()

df['topic'].value_counts()

df['sign'].value_counts()

df.isna().sum()

df[df.duplicated(keep=False)]

df.duplicated().sum()

df = df.drop_duplicates(
    subset=['id', 'text'],
    keep='first'
)

df = df[df['topic'].ne('indUnk')].copy()

# Mapping topic -> topic_general (lebih general)
topic_map = {
    # Education & Academia
    'Student': 'Education & Academia',
    'Education': 'Education & Academia',
    'Science': 'Education & Academia',
    'Publishing': 'Education & Academia',
    'Museums-Libraries': 'Education & Academia',
    'Engineering': 'Education & Academia',  # Anda bisa pindah ke Tech/Industry jika mau

    # Technology & Internet
    'Technology': 'Technology & Internet',
    'Internet': 'Technology & Internet',
    'Telecommunications': 'Technology & Internet',
    'Biotech': 'Technology & Internet',

    # Business & Finance
    'BusinessServices': 'Business & Finance',
    'Consulting': 'Business & Finance',
    'Accounting': 'Business & Finance',
    'Banking': 'Business & Finance',
    'InvestmentBanking': 'Business & Finance',
    'Marketing': 'Business & Finance',
    'Advertising': 'Business & Finance',
    'RealEstate': 'Business & Finance',
    'HumanResources': 'Business & Finance',

    # Media, Arts & Culture
    'Arts': 'Media, Arts & Culture',
    'Communications-Media': 'Media, Arts & Culture',
    'Fashion': 'Media, Arts & Culture',
    'Sports-Recreation': 'Media, Arts & Culture',
    'Tourism': 'Media, Arts & Culture',

    # Government & Law
    'Government': 'Government & Law',
    'Law': 'Government & Law',
    'LawEnforcement-Security': 'Government & Law',
    'Military': 'Government & Law',

    # Industry & Infrastructure
    'Manufacturing': 'Industry & Infrastructure',
    'Construction': 'Industry & Infrastructure',
    'Transportation': 'Industry & Infrastructure',
    'Automotive': 'Industry & Infrastructure',
    'Chemicals': 'Industry & Infrastructure',
    'Agriculture': 'Industry & Infrastructure',
    'Maritime': 'Industry & Infrastructure',
    'Architecture': 'Industry & Infrastructure',

    # Environment & Social
    'Environment': 'Environment & Social',
    'Non-Profit': 'Environment & Social',
    'Religion': 'Environment & Social',
}

# Buat kolom baru: topic_general
df['topic_general'] = df['topic'].map(topic_map).fillna('Other')

# Audit hasil: distribusi topik general
print("Jumlah data setelah hapus indUnk:", len(df))
print("\nDistribusi topic_general:")
print(df['topic_general'].value_counts())

"""## Visualisasi"""

import matplotlib.pyplot as plt

gender_counts = df['gender'].value_counts()

plt.figure(figsize=(6,6))
plt.pie(
    gender_counts,
    labels=gender_counts.index,
    autopct='%1.1f%%',
    startangle=90
)
plt.title('Distribusi Gender')
plt.axis('equal')
plt.show()

plt.figure(figsize=(8,5))
plt.hist(df['age'], bins=15, edgecolor='black')
plt.xlabel('Age')
plt.ylabel('Frekuensi')
plt.title('Distribusi Umur (Histogram)')
plt.show()

topic_counts = df['topic_general'].value_counts()

plt.figure(figsize=(10,5))
plt.bar(topic_counts.index, topic_counts.values)
plt.xlabel('Topic')
plt.ylabel('Jumlah Data')
plt.title('Distribusi Topic')
plt.xticks(rotation=45, ha='right')
plt.show()

sign_counts = df['sign'].value_counts()

plt.figure(figsize=(10,5))
plt.bar(sign_counts.index, sign_counts.values)
plt.xlabel('Zodiac Sign')
plt.ylabel('Jumlah Data')
plt.title('Distribusi Zodiac Sign')
plt.xticks(rotation=45)
plt.show()

pd.crosstab(df['topic_general'], df['gender']).plot(
    kind='bar',
    stacked=True,
    figsize=(10,6)
)

plt.title('Distribusi Topic berdasarkan Gender')
plt.xlabel('Topic')
plt.ylabel('Jumlah Data')
plt.xticks(rotation=45)
plt.show()

"""## Pre-Processing"""

import re

def normalize_text(s: str) -> str:
    if pd.isna(s):
        return ""
    s = str(s).lower()
    s = re.sub(r"http\S+|www\.\S+", " ", s)      # URL
    s = re.sub(r"\S+@\S+\.\S+", " ", s)          # email
    s = re.sub(r"\s+", " ", s).strip()           # spasi berantakan
    return s

from sklearn.model_selection import train_test_split

# buang baris tanpa target / teks kosong
df_pp = df.copy()
df_pp = df_pp[df_pp['topic_general'].notna()]
df_pp = df_pp[df_pp['text'].notna()]
df_pp['text_norm'] = df_pp['text'].apply(normalize_text)
df_pp = df_pp[df_pp['text_norm'].str.len() > 0]

X = df_pp['text_norm']
y = df_pp['topic_general']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

train_df = pd.DataFrame({'text_norm': X_train, 'topic_general': y_train}).reset_index(drop=True)
test_df  = pd.DataFrame({'text_norm': X_test,  'topic_general': y_test}).reset_index(drop=True)

import spacy

nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])

def lemmatize_remove_stopwords(text: str) -> str:
    doc = nlp(text)
    tokens = []
    for t in doc:
        if t.is_space or t.is_punct:
            continue
        if t.is_stop:
            continue
        lemma = t.lemma_.strip()
        if not lemma:
            continue
        # keep hanya kata alfabet (opsional: Anda bisa izinkan angka)
        if not lemma.isalpha():
            continue
        tokens.append(lemma)
    return " ".join(tokens)

train_df['text_pp'] = train_df['text_norm'].apply(lemmatize_remove_stopwords)
test_df['text_pp']  = test_df['text_norm'].apply(lemmatize_remove_stopwords)

# Buang yang jadi kosong setelah stopword removal + lemma
train_df = train_df[train_df['text_pp'].str.len() > 0].reset_index(drop=True)
test_df  = test_df[test_df['text_pp'].str.len() > 0].reset_index(drop=True)

print("Train size:", len(train_df))
print("Test size :", len(test_df))

print("\nContoh sebelum vs sesudah preprocessing:")
display(train_df[['text_norm', 'text_pp']].head(5))

import os

out_dir = "saved_artifacts"
os.makedirs(out_dir, exist_ok=True)

# Simpan CSV
train_df.to_csv(os.path.join(out_dir, "train_lemmatized.csv"), index=False, encoding="utf-8")
test_df.to_csv(os.path.join(out_dir, "test_lemmatized.csv"), index=False, encoding="utf-8")

print("Saved:", os.path.join(out_dir, "train_lemmatized.csv"))
print("Saved:", os.path.join(out_dir, "test_lemmatized.csv"))

"""## Modeling"""

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

lsa_pipeline = Pipeline([
    ("tfidf", TfidfVectorizer(
        ngram_range=(1,2),
        min_df=5,
        max_df=0.9
    )),
    ("svd", TruncatedSVD(
        n_components=300,
        random_state=42
    )),
    ("clf", LogisticRegression(
        max_iter=3000,
        class_weight="balanced",
        n_jobs=-1
    ))
])

lsa_pipeline.fit(train_df['text_pp'], train_df['topic_general'])
lsa_pred = lsa_pipeline.predict(test_df['text_pp'])

print("=== LSA ===")
print(classification_report(
    test_df['topic_general'], lsa_pred, digits=4
))

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

lda_pipeline = Pipeline([
    ("bow", CountVectorizer(
        ngram_range=(1,2),
        min_df=5,
        max_df=0.9
    )),
    ("lda", LatentDirichletAllocation(
        n_components=30,
        learning_method="online",
        random_state=42
    )),
    ("clf", LogisticRegression(
        max_iter=3000,
        class_weight="balanced",
        n_jobs=-1
    ))
])

lda_pipeline.fit(train_df['text_pp'], train_df['topic_general'])
lda_pred = lda_pipeline.predict(test_df['text_pp'])

print("=== LDA ===")
print(classification_report(
    test_df['topic_general'], lda_pred, digits=4
))

import joblib

joblib.dump(lsa_pipeline, os.path.join(out_dir, "model_lsa.joblib"))
joblib.dump(lda_pipeline, os.path.join(out_dir, "model_lda.joblib"))

print("Saved LSA & LDA models in:", out_dir)

import joblib
import os

out_dir = "saved_artifacts"

lsa_pipeline = joblib.load(os.path.join(out_dir, "model_lsa.joblib"))
lda_pipeline = joblib.load(os.path.join(out_dir, "model_lda.joblib"))

import pandas as pd

test_df = pd.read_csv(os.path.join(out_dir, "test_lemmatized.csv"))

X_test = test_df['text_pp']
y_test = test_df['topic_general']

from sklearn.metrics import classification_report, confusion_matrix

# ===== LSA =====
lsa_pred = lsa_pipeline.predict(X_test)

print("=== Evaluation: LSA (Loaded Model) ===")
print(classification_report(y_test, lsa_pred, digits=4))

print("Confusion Matrix (LSA):")
print(confusion_matrix(y_test, lsa_pred))


# ===== LDA =====
lda_pred = lda_pipeline.predict(X_test)

print("\n=== Evaluation: LDA (Loaded Model) ===")
print(classification_report(y_test, lda_pred, digits=4))

print("Confusion Matrix (LDA):")
print(confusion_matrix(y_test, lda_pred))

"""### BERT"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import Trainer, TrainingArguments
from datasets import Dataset
import numpy as np
from sklearn.metrics import f1_score

label2id = {l:i for i,l in enumerate(train_df['topic_general'].unique())}
id2label = {i:l for l,i in label2id.items()}

train_bert = Dataset.from_pandas(
    train_df[['text_norm','topic_general']]
    .rename(columns={'text_norm':'text','topic_general':'label'})
)
test_bert = Dataset.from_pandas(
    test_df[['text_norm','topic_general']]
    .rename(columns={'text_norm':'text','topic_general':'label'})
)

train_bert = train_bert.map(lambda x: {'label': label2id[x['label']]})
test_bert  = test_bert.map(lambda x: {'label': label2id[x['label']]})

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def tokenize(batch):
    return tokenizer(
        batch["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )

train_bert = train_bert.map(tokenize, batched=True)
test_bert  = test_bert.map(tokenize, batched=True)

model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

def compute_metrics(pred):
    logits, labels = pred
    preds = np.argmax(logits, axis=1)
    return {
        "macro_f1": f1_score(labels, preds, average="macro")
    }

args = TrainingArguments(
    output_dir="./bert_out",
    evaluation_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    logging_steps=200,
    load_best_model_at_end=True,
    metric_for_best_model="macro_f1"
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_bert,
    eval_dataset=test_bert,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()